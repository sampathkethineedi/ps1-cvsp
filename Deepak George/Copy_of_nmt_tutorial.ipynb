{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of nmt_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNxDp3Me4zXx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# NMT-Keras tutorial\n",
        "---\n",
        "\n",
        "This notebook describes, step by step, how to build a neural machine translation model with NMT-Keras. The tutorial is organized in different sections:\n",
        "\n",
        "\n",
        "1. Create a Dataset instance, in order to properly manage the data. \n",
        "2. Create and train the Neural Translation Model in the training data.\n",
        "3. Apply the trained model on new (unseen) data.\n",
        "\n",
        "All these steps are automatically run by the toolkit. But, to learn and understand the full process, it is didactic to follow this tutorial.\n",
        "\n",
        "\n",
        "So, let's start installing the toolkit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mtJWLes5JO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4e1e47b-99fd-4344-da5e-fcb769a61c8b"
      },
      "source": [
        "!pip install update pip\n",
        "!pip uninstall -y keras  # Avoid crashes with pre-installed packages\n",
        "!git clone https://github.com/lvapeab/nmt-keras\n",
        "import os\n",
        "os.chdir('nmt-keras')\n",
        "!pip install -e .\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting update\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/c4/dfe8a392edd35cc635c35cd3b20df6a746aacdeb39b685d1668b56bf819b/update-0.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Collecting style==1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/0b/6be2071e20c621e7beb01b86e8474c2ec344a9750ba5315886f24d6e7386/style-1.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: style, update\n",
            "Successfully installed style-1.1.0 update-0.0.1\n",
            "Uninstalling Keras-2.3.1:\n",
            "  Successfully uninstalled Keras-2.3.1\n",
            "Cloning into 'nmt-keras'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 4736 (delta 0), reused 0 (delta 0), pack-reused 4730\u001b[K\n",
            "Receiving objects: 100% (4736/4736), 5.69 MiB | 4.46 MiB/s, done.\n",
            "Resolving deltas: 100% (3215/3215), done.\n",
            "Obtaining file:///content/nmt-keras\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (0.16.0)\n",
            "Collecting keras@ https://github.com/MarcBS/keras/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/MarcBS/keras/archive/master.zip\n",
            "\u001b[K     / 107.9MB 1.2MB/s\n",
            "\u001b[?25hRequirement already satisfied: keras_applications in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (1.0.8)\n",
            "Requirement already satisfied: keras_preprocessing in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (2.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (3.2.1)\n",
            "Collecting multimodal-keras-wrapper\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b6/d9e00359c1f232cc29366547778006571771e8372430661f9fc2807af382/multimodal_keras_wrapper-3.1.6-py3-none-any.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (1.18.5)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (0.16.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (1.12.0)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (3.4.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (1.0.4)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/4b/6c7a0b26a48d88f56573d11aa5058808fe0d36ba40951287894f943556b5/sacrebleu-1.4.10-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from nmt-keras==0.6) (1.4.1)\n",
            "Collecting tensorflow<2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/36/9a02e27f0ec248b676a380ffe910c1858e3af3027c0d4d513dd0b56a5613/tensorflow-1.15.3-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 60kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras@ https://github.com/MarcBS/keras/archive/master.zip->nmt-keras==0.6) (3.13)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nmt-keras==0.6) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nmt-keras==0.6) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nmt-keras==0.6) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nmt-keras==0.6) (0.10.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from multimodal-keras-wrapper->nmt-keras==0.6) (0.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from multimodal-keras-wrapper->nmt-keras==0.6) (0.29.20)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from multimodal-keras-wrapper->nmt-keras==0.6) (0.10.0)\n",
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->nmt-keras==0.6) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->nmt-keras==0.6) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->nmt-keras==0.6) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->nmt-keras==0.6) (2.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nmt-keras==0.6) (0.15.1)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->nmt-keras==0.6) (2.7.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nmt-keras==0.6) (2018.9)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses->nmt-keras==0.6) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->nmt-keras==0.6) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses->nmt-keras==0.6) (4.41.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2->nmt-keras==0.6) (3.2.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2->nmt-keras==0.6) (1.29.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 38.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2->nmt-keras==0.6) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2->nmt-keras==0.6) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2->nmt-keras==0.6) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2->nmt-keras==0.6) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2->nmt-keras==0.6) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2->nmt-keras==0.6) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2->nmt-keras==0.6) (0.9.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->nmt-keras==0.6) (4.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (47.3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (3.1.0)\n",
            "Building wheels for collected packages: keras, sacremoses, gast\n",
            "  Building wheel for keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras: filename=Keras-2.3.1.1-cp36-none-any.whl size=487500 sha256=af96975fa31524f93cd4109d4b5e1ece897f92aae238c6e90733b2e2a8c6a37e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fq1pm2za/wheels/82/f8/db/7c0c999dced9850abb60944d255a31dbdf10f76f645454b715\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=1480937f8d7fe1d5e30a62cb8ff244226e4cebd8db42977f112cdd7034763d64\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=c5a4efb6de1b445f2cd8d1f46591059d61c596159f3fcda44bc0ccf1ecfa2cc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built keras sacremoses gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras, sacremoses, portalocker, sacrebleu, subword-nmt, multimodal-keras-wrapper, tensorflow-estimator, tensorboard, gast, tensorflow, nmt-keras\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "  Running setup.py develop for nmt-keras\n",
            "Successfully installed gast-0.2.2 keras-2.3.1.1 multimodal-keras-wrapper-3.1.6 nmt-keras portalocker-1.7.0 sacrebleu-1.4.10 sacremoses-0.0.43 subword-nmt-0.3.7 tensorboard-1.15.0 tensorflow-1.15.3 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSSh7bps4y1U",
        "colab_type": "text"
      },
      "source": [
        "## 1. Building a Dataset model\n",
        "First, we are creating a [Dataset](https://github.com/MarcBS/multimodal_keras_wrapper/keras_wrapper/dataset.py) object (from the [Multimodal Keras Wrapper](https://github.com/MarcBS/multimodal_keras_wrapper) library). This object will be the interface between our data (text files) and the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiY2fUFU83Rx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a70bc176-6bb4-4bcc-bd71-6993c3300b8a"
      },
      "source": [
        "from keras_wrapper.dataset import Dataset, saveDataset\n",
        "from data_engine.prepare_data import keep_n_captions\n",
        "ds = Dataset('tutorial_dataset', 'tutorial', silence=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ify05BL8_Rj",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the empty dataset, we must indicate its inputs and outputs. In our case, we'll have two different inputs and one single output:\n",
        "\n",
        "1. Outputs:\n",
        "**target_text**: Sentences in our target language.\n",
        "\n",
        "2. Inputs:\n",
        "**source_text**: Sentences in the source language.\n",
        "\n",
        "**state_below**: Sentences in the target language, but shifted one position to the right (for teacher-forcing training of the model).\n",
        "\n",
        "For setting up the outputs, we use the setOutputs function, with the appropriate parameters. Note that, when we are building the dataset for the training split, we build the vocabulary (up to 30000 words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1A8D0qn9IeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "04276068-382c-42b8-e68f-bfd529986376"
      },
      "source": [
        "ds.setOutput('/content/drive/My Drive/parallel-dataset/train.en',\n",
        "             'train',\n",
        "             type='text',\n",
        "             id='target_text',\n",
        "             tokenization='tokenize_none',\n",
        "             build_vocabulary=True,\n",
        "             pad_on_batch=True,\n",
        "             sample_weights=True,\n",
        "             max_text_len=30,\n",
        "             max_words=50000,\n",
        "             min_occ=0)\n",
        "\n",
        "ds.setOutput('/content/drive/My Drive/parallel-dataset/val.en',\n",
        "             'val',\n",
        "             type='text',\n",
        "             id='target_text',\n",
        "             pad_on_batch=True,\n",
        "             tokenization='tokenize_none',\n",
        "             sample_weights=True,\n",
        "             max_text_len=30,\n",
        "             max_words=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[22/06/2020 15:57:56] \tApplying tokenization function: \"tokenize_none\".\n",
            "[22/06/2020 15:57:56] Creating vocabulary for data with data_id 'target_text'.\n",
            "[22/06/2020 15:57:57] \t Total: 15457 unique words in 29000 sentences with a total of 345022 words.\n",
            "[22/06/2020 15:57:57] Creating dictionary of 50000 most common words, covering 100.0% of the text.\n",
            "[22/06/2020 15:57:57] Loaded \"train\" set outputs of data_type \"text\" with data_id \"target_text\" and length 29000.\n",
            "[22/06/2020 15:57:57] \tApplying tokenization function: \"tokenize_none\".\n",
            "[22/06/2020 15:57:57] Loaded \"val\" set outputs of data_type \"text\" with data_id \"target_text\" and length 1014.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jipVyl7f9NTz",
        "colab_type": "text"
      },
      "source": [
        "Similarly, we introduce the source text data, with the setInputs function. Again, when building the training split, we must construct the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f72G9mHX9PPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "f7df0cdc-ba0d-40af-d63f-e805dfab732c"
      },
      "source": [
        "ds.setInput('/content/drive/My Drive/parallel-dataset/train.de',\n",
        "            'train',\n",
        "            type='text',\n",
        "            id='source_text',\n",
        "            pad_on_batch=True,\n",
        "            tokenization='tokenize_none',\n",
        "            build_vocabulary=True,\n",
        "            fill='end',\n",
        "            max_text_len=30,\n",
        "            max_words=30000,\n",
        "            min_occ=0)\n",
        "ds.setInput('/content/drive/My Drive/parallel-dataset/val.de',\n",
        "            'val',\n",
        "            type='text',\n",
        "            id='source_text',\n",
        "            pad_on_batch=True,\n",
        "            tokenization='tokenize_none',\n",
        "            fill='end',\n",
        "            max_text_len=30,\n",
        "            min_occ=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:22] \tApplying tokenization function: \"tokenize_none\".\n",
            "[20/06/2020 13:40:22] Creating vocabulary for data with data_id 'source_text'.\n",
            "[20/06/2020 13:40:22] \t Total: 24907 unique words in 29000 sentences with a total of 322380 words.\n",
            "[20/06/2020 13:40:22] Creating dictionary of 30000 most common words, covering 100.0% of the text.\n",
            "[20/06/2020 13:40:22] Loaded \"train\" set inputs of data_type \"text\" with data_id \"source_text\" and length 29000.\n",
            "[20/06/2020 13:40:22] \tApplying tokenization function: \"tokenize_none\".\n",
            "[20/06/2020 13:40:22] Loaded \"val\" set inputs of data_type \"text\" with data_id \"source_text\" and length 1014.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPzUJLYs-Cao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f9740047-61aa-49f6-94a8-15e6c239a3a3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHvZimmm9U0p",
        "colab_type": "text"
      },
      "source": [
        "...and for the 'state_below' data. Note that: 1) The offset flat is set to 1, which means that the text will be shifted to the right 1 position. 2) During sampling time, we won't have this input. Hence, we 'hack' the dataset model by inserting an artificial input, of type 'ghost' for the validation split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiTM3y449ZFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "e024b491-bb95-45c2-e9cf-4123de41a390"
      },
      "source": [
        "ds.setInput('/content/drive/My Drive/parallel-dataset/train.en',\n",
        "            'train',\n",
        "            type='text',\n",
        "            id='state_below',\n",
        "            required=False,\n",
        "            tokenization='tokenize_none',\n",
        "            pad_on_batch=True,\n",
        "            build_vocabulary='target_text',\n",
        "            offset=1,\n",
        "            fill='end',\n",
        "            max_text_len=30,\n",
        "            max_words=30000)\n",
        "ds.setInput(None,\n",
        "            'val',\n",
        "            type='ghost',\n",
        "            id='state_below',\n",
        "            required=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:31] \tApplying tokenization function: \"tokenize_none\".\n",
            "[20/06/2020 13:40:31] \tReusing vocabulary named \"target_text\" for data with data_id \"state_below\".\n",
            "[20/06/2020 13:40:31] Loaded \"train\" set inputs of data_type \"text\" with data_id \"state_below\" and length 29000.\n",
            "[20/06/2020 13:40:31] Loaded \"val\" set inputs of data_type \"ghost\" with data_id \"state_below\" and length 1014.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H-1qb1ksD5Y",
        "colab_type": "text"
      },
      "source": [
        "We can also keep the literal source words (for replacing unknown words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkC4AUjGsLYd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2e51f53c-73c6-4c1b-a41a-d2bf760c6f07"
      },
      "source": [
        "  for split, input_text_filename in zip(['train', 'val'], ['/content/drive/My Drive/parallel-dataset/train.de', '/content/drive/My Drive/parallel-dataset/val.de']):\n",
        "    ds.setRawInput(input_text_filename,\n",
        "                  split,\n",
        "                  type='file-name',\n",
        "                  id='raw_source_text',\n",
        "                  overwrite_split=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:36] Loaded \"train\" set inputs of type \"file-name\" with id \"raw_source_text\".\n",
            "[20/06/2020 13:40:36] Loaded \"val\" set inputs of type \"file-name\" with id \"raw_source_text\".\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcYiuysd9cK4",
        "colab_type": "text"
      },
      "source": [
        "We also need to match the references with the inputs. Since we only have one reference per input sample, we set `repeat=1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s94NfwBl9sZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1ad39f3b-0656-4e70-d80e-5c0624a5d7b3"
      },
      "source": [
        "keep_n_captions(ds, repeat=1, n=1, set_names=['val'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:39] Keeping 1 captions per input on the val set.\n",
            "[20/06/2020 13:40:39] Samples reduced to 1014 in val set.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDYhlBpR9vgi",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can save our dataset instance for using in other experiments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS5vZ5zp9tkG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "1e307c2b-2d21-4077-e741-05af6a1cb26e"
      },
      "source": [
        "saveDataset(ds, 'datasets')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:43] <<< creating directory datasets ... >>>\n",
            "[20/06/2020 13:40:43] <<< Saving Dataset instance to datasets/Dataset_tutorial_dataset.pkl ... >>>\n",
            "[20/06/2020 13:40:43] <<< Dataset instance saved >>>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj8aYICU-eLH",
        "colab_type": "text"
      },
      "source": [
        "## 2. Creating and training a Neural Translation Model\n",
        "Now, we'll create and train a Neural Machine Translation (NMT) model. Since there is a significant number of hyperparameters, we'll use the default ones, specified in the `config.py` file. Note that almost every hardcoded parameter is automatically set from config if we run  `main.py `.\n",
        "\n",
        "We'll create an `'AttentionRNNEncoderDecoder'` (a LSTM encoder-decoder with attention mechanism). Refer to the [`model_zoo.py`](https://github.com/lvapeab/nmt-keras/blob/master/nmt_keras/model_zoo.py) file for other models (e.g. Transformer). \n",
        "\n",
        "So first, let's import the model and the hyperparameters. We'll also load the dataset we stored in the previous section (not necessary as it is in memory, but as a demonstration):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TszghyVO_M0B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "36947851-7b1a-4cb6-94a1-5c99c8cbd7ac"
      },
      "source": [
        "from config import load_parameters\n",
        "from nmt_keras.model_zoo import TranslationModel\n",
        "from keras_wrapper.cnn_model import loadModel\n",
        "from keras_wrapper.dataset import loadDataset\n",
        "from keras_wrapper.extra.callbacks import PrintPerformanceMetricOnEpochEndOrEachNUpdates\n",
        "params = load_parameters()\n",
        "dataset = loadDataset('datasets/Dataset_tutorial_dataset.pkl')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:48] <<< Loading Dataset instance from datasets/Dataset_tutorial_dataset.pkl ... >>>\n",
            "[20/06/2020 13:40:48] <<< Dataset instance loaded >>>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MHVvMDYFmcQ",
        "colab_type": "text"
      },
      "source": [
        "Since the number of words in the dataset may be unknown beforehand, we must update the params information according to the dataset instance:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7FIJGybFm7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params['INPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len['source_text']\n",
        "params['OUTPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len['target_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3a9XanxFpp7",
        "colab_type": "text"
      },
      "source": [
        "Now, we create a `TranslationModel` instance:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RYqrU6VFr8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77a63f12-84bc-4d8e-f4c4-3e65634fff17"
      },
      "source": [
        "params['MODEL_TYPE'] = 'AttentionRNNEncoderDecoder' #  Supported models: 'AttentionRNNEncoderDecoder' and 'Transformer'.\n",
        "nmt_model = TranslationModel(params,\n",
        "                             model_type=params['MODEL_TYPE'], \n",
        "                             model_name='tutorial_model',\n",
        "                             vocabularies=dataset.vocabulary,\n",
        "                             store_path='trained_models/tutorial_model/',\n",
        "                             verbose=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:55] <<< Building AttentionRNNEncoderDecoder Translation_Model >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:650: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:55] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:650: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4786: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:55] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4786: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:157: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:55] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:157: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3561: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:56] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3561: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------\n",
            "\t\tTranslationModel instance\n",
            "-----------------------------------------------------------------------------------\n",
            "_model_type: AttentionRNNEncoderDecoder\n",
            "name: tutorial_model\n",
            "model_path: trained_models/tutorial_model/\n",
            "verbose: True\n",
            "\n",
            "Params:\n",
            "\tACCUMULATE_GRADIENTS: 1\n",
            "\tADDITIONAL_OUTPUT_MERGE_MODE: Add\n",
            "\tALIGN_FROM_RAW: True\n",
            "\tALPHA_FACTOR: 0.6\n",
            "\tAMSGRAD: False\n",
            "\tAPPLY_DETOKENIZATION: False\n",
            "\tATTENTION_DROPOUT_P: 0.0\n",
            "\tATTENTION_MODE: add\n",
            "\tATTENTION_SIZE: 32\n",
            "\tBATCH_NORMALIZATION_MODE: 1\n",
            "\tBATCH_SIZE: 50\n",
            "\tBEAM_SEARCH: True\n",
            "\tBEAM_SIZE: 6\n",
            "\tBETA_1: 0.9\n",
            "\tBETA_2: 0.999\n",
            "\tBIDIRECTIONAL_DEEP_ENCODER: True\n",
            "\tBIDIRECTIONAL_ENCODER: True\n",
            "\tBIDIRECTIONAL_MERGE_MODE: concat\n",
            "\tBPE_CODES_PATH: examples/EuTrans//training_codes.joint\n",
            "\tCLASSIFIER_ACTIVATION: softmax\n",
            "\tCLIP_C: 5.0\n",
            "\tCLIP_V: 0.0\n",
            "\tCOVERAGE_NORM_FACTOR: 0.2\n",
            "\tCOVERAGE_PENALTY: False\n",
            "\tDATASET_NAME: EuTrans\n",
            "\tDATASET_STORE_PATH: datasets/\n",
            "\tDATA_AUGMENTATION: False\n",
            "\tDATA_ROOT_PATH: examples/EuTrans/\n",
            "\tDECODER_HIDDEN_SIZE: 32\n",
            "\tDECODER_RNN_TYPE: ConditionalLSTM\n",
            "\tDEEP_OUTPUT_LAYERS: [('linear', 32)]\n",
            "\tDETOKENIZATION_METHOD: detokenize_none\n",
            "\tDOUBLE_STOCHASTIC_ATTENTION_REG: 0.0\n",
            "\tDROPOUT_P: 0.0\n",
            "\tEARLY_STOP: True\n",
            "\tEMBEDDINGS_FREQ: 1\n",
            "\tENCODER_HIDDEN_SIZE: 32\n",
            "\tENCODER_RNN_TYPE: LSTM\n",
            "\tEPOCHS_FOR_SAVE: 1\n",
            "\tEPSILON: 1e-08\n",
            "\tEVAL_EACH: 1\n",
            "\tEVAL_EACH_EPOCHS: True\n",
            "\tEVAL_ON_SETS: ['val']\n",
            "\tEXTRA_NAME: \n",
            "\tFF_SIZE: 128\n",
            "\tFILL: end\n",
            "\tFORCE_RELOAD_VOCABULARY: False\n",
            "\tGLOSSARY: None\n",
            "\tGRU_RESET_AFTER: True\n",
            "\tHEURISTIC: 0\n",
            "\tHOMOGENEOUS_BATCHES: False\n",
            "\tINIT_ATT: glorot_uniform\n",
            "\tINIT_FUNCTION: glorot_uniform\n",
            "\tINIT_LAYERS: ['tanh']\n",
            "\tINNER_INIT: orthogonal\n",
            "\tINPUTS_IDS_DATASET: ['source_text', 'state_below']\n",
            "\tINPUTS_IDS_MODEL: ['source_text', 'state_below']\n",
            "\tINPUTS_TYPES_DATASET: ['text-features', 'text-features']\n",
            "\tINPUT_VOCABULARY_SIZE: 24910\n",
            "\tJOINT_BATCHES: 4\n",
            "\tKERAS_METRICS: ['perplexity']\n",
            "\tLABEL_SMOOTHING: 0.0\n",
            "\tLENGTH_NORM_FACTOR: 0.2\n",
            "\tLENGTH_PENALTY: False\n",
            "\tLOG_DIR: tensorboard_logs\n",
            "\tLOSS: categorical_crossentropy\n",
            "\tLR: 0.001\n",
            "\tLR_DECAY: None\n",
            "\tLR_GAMMA: 0.8\n",
            "\tLR_HALF_LIFE: 100\n",
            "\tLR_REDUCER_EXP_BASE: -0.5\n",
            "\tLR_REDUCER_TYPE: exponential\n",
            "\tLR_REDUCE_EACH_EPOCHS: False\n",
            "\tLR_START_REDUCTION_ON_EPOCH: 0\n",
            "\tMAPPING: examples/EuTrans//mapping.es_en.pkl\n",
            "\tMAXLEN_GIVEN_X: True\n",
            "\tMAXLEN_GIVEN_X_FACTOR: 2\n",
            "\tMAX_EPOCH: 500\n",
            "\tMAX_INPUT_TEXT_LEN: 50\n",
            "\tMAX_OUTPUT_TEXT_LEN: 50\n",
            "\tMAX_OUTPUT_TEXT_LEN_TEST: 150\n",
            "\tMAX_PLOT_Y: 100.0\n",
            "\tMETRICS: ['sacrebleu', 'perplexity']\n",
            "\tMINLEN_GIVEN_X: True\n",
            "\tMINLEN_GIVEN_X_FACTOR: 3\n",
            "\tMIN_DELTA: 0.0\n",
            "\tMIN_LR: 1e-09\n",
            "\tMIN_OCCURRENCES_INPUT_VOCAB: 0\n",
            "\tMIN_OCCURRENCES_OUTPUT_VOCAB: 0\n",
            "\tMODE: training\n",
            "\tMODEL_NAME: EuTrans_esen_AttentionRNNEncoderDecoder_src_emb_32_bidir_True_enc_LSTM_32_dec_ConditionalLSTM_32_deepout_linear_trg_emb_32_Adam_0.001\n",
            "\tMODEL_SIZE: 32\n",
            "\tMODEL_TYPE: AttentionRNNEncoderDecoder\n",
            "\tMOMENTUM: 0.0\n",
            "\tMULTIHEAD_ATTENTION_ACTIVATION: linear\n",
            "\tNESTEROV_MOMENTUM: False\n",
            "\tNOISE_AMOUNT: 0.01\n",
            "\tNORMALIZE_SAMPLING: False\n",
            "\tN_GPUS: 1\n",
            "\tN_HEADS: 8\n",
            "\tN_LAYERS_DECODER: 1\n",
            "\tN_LAYERS_ENCODER: 1\n",
            "\tN_SAMPLES: 5\n",
            "\tOPTIMIZED_SEARCH: True\n",
            "\tOPTIMIZER: Adam\n",
            "\tOUTPUTS_IDS_DATASET: ['target_text']\n",
            "\tOUTPUTS_IDS_MODEL: ['target_text']\n",
            "\tOUTPUTS_TYPES_DATASET: ['text-features']\n",
            "\tOUTPUT_VOCABULARY_SIZE: 15460\n",
            "\tPAD_ON_BATCH: True\n",
            "\tPARALLEL_LOADERS: 1\n",
            "\tPATIENCE: 10\n",
            "\tPLOT_EVALUATION: False\n",
            "\tPOS_UNK: True\n",
            "\tREBUILD_DATASET: True\n",
            "\tRECURRENT_DROPOUT_P: 0.0\n",
            "\tRECURRENT_INPUT_DROPOUT_P: 0.0\n",
            "\tRECURRENT_WEIGHT_DECAY: 0.0\n",
            "\tREGULARIZATION_FN: L2\n",
            "\tRELOAD: 0\n",
            "\tRELOAD_EPOCH: True\n",
            "\tRHO: 0.9\n",
            "\tSAMPLE_EACH_UPDATES: 300\n",
            "\tSAMPLE_ON_SETS: ['train', 'val']\n",
            "\tSAMPLE_WEIGHTS: True\n",
            "\tSAMPLING: max_likelihood\n",
            "\tSAMPLING_SAVE_MODE: list\n",
            "\tSAVE_EACH_EVALUATION: True\n",
            "\tSCALE_SOURCE_WORD_EMBEDDINGS: False\n",
            "\tSCALE_TARGET_WORD_EMBEDDINGS: False\n",
            "\tSEARCH_PRUNING: False\n",
            "\tSKIP_VECTORS_HIDDEN_SIZE: 32\n",
            "\tSKIP_VECTORS_SHARED_ACTIVATION: tanh\n",
            "\tSOURCE_TEXT_EMBEDDING_SIZE: 32\n",
            "\tSRC_LAN: es\n",
            "\tSRC_PRETRAINED_VECTORS: None\n",
            "\tSRC_PRETRAINED_VECTORS_TRAINABLE: True\n",
            "\tSTART_EVAL_ON_EPOCH: 1\n",
            "\tSTART_SAMPLING_ON_EPOCH: 1\n",
            "\tSTOP_METRIC: Bleu_4\n",
            "\tSTORE_PATH: trained_models/EuTrans_esen_AttentionRNNEncoderDecoder_src_emb_32_bidir_True_enc_LSTM_32_dec_ConditionalLSTM_32_deepout_linear_trg_emb_32_Adam_0.001/\n",
            "\tTARGET_TEXT_EMBEDDING_SIZE: 32\n",
            "\tTASK_NAME: EuTrans\n",
            "\tTEMPERATURE: 1\n",
            "\tTENSORBOARD: True\n",
            "\tTEXT_FILES: {'train': 'training.', 'val': 'dev.', 'test': 'test.'}\n",
            "\tTIE_EMBEDDINGS: False\n",
            "\tTOKENIZATION_METHOD: tokenize_none\n",
            "\tTOKENIZE_HYPOTHESES: True\n",
            "\tTOKENIZE_REFERENCES: True\n",
            "\tTRAINABLE_DECODER: True\n",
            "\tTRAINABLE_ENCODER: True\n",
            "\tTRAIN_ON_TRAINVAL: False\n",
            "\tTRG_LAN: en\n",
            "\tTRG_PRETRAINED_VECTORS: None\n",
            "\tTRG_PRETRAINED_VECTORS_TRAINABLE: True\n",
            "\tUSE_BATCH_NORMALIZATION: True\n",
            "\tUSE_CUDNN: False\n",
            "\tUSE_L1: False\n",
            "\tUSE_L2: False\n",
            "\tUSE_NOISE: False\n",
            "\tUSE_PRELU: False\n",
            "\tUSE_TF_OPTIMIZER: True\n",
            "\tVERBOSE: 1\n",
            "\tWARMUP_EXP: -1.5\n",
            "\tWEIGHT_DECAY: 0.0001\n",
            "\tWRITE_VALID_SAMPLES: True\n",
            "-----------------------------------------------------------------------------------\n",
            "Model: \"tutorial_model_training\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "source_text (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "source_word_embedding (Embeddin (None, None, 32)     797120      source_text[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "src_embedding_batch_normalizati (None, None, 32)     128         source_word_embedding[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "remove_mask_1 (RemoveMask)      (None, None, 32)     0           src_embedding_batch_normalization\n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_encoder_LSTM (Bid (None, None, 64)     16640       remove_mask_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "annotations_batch_normalization (None, None, 64)     256         bidirectional_encoder_LSTM[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "source_text_mask (GetMask)      (None, None, 32)     0           src_embedding_batch_normalization\n",
            "__________________________________________________________________________________________________\n",
            "annotations (ApplyMask)         (None, None, 64)     0           annotations_batch_normalization[0\n",
            "                                                                 source_text_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "state_below (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "ctx_mean (MaskedMean)           (None, 64)           0           annotations[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "target_word_embedding (Embeddin (None, None, 32)     494720      state_below[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "initial_state (Dense)           (None, 32)           2080        ctx_mean[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "initial_memory (Dense)          (None, 32)           2080        ctx_mean[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "state_below_batch_normalization (None, None, 32)     128         target_word_embedding[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "initial_state_batch_normalizati (None, 32)           128         initial_state[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "initial_memory_batch_normalizat (None, 32)           128         initial_memory[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_AttConditionalLSTMCond  [(None, None, 32), ( 23873       state_below_batch_normalization[0\n",
            "                                                                 annotations[0][0]                \n",
            "                                                                 initial_state_batch_normalization\n",
            "                                                                 initial_memory_batch_normalizatio\n",
            "__________________________________________________________________________________________________\n",
            "proj_h0_batch_normalization (Ba (None, None, 32)     128         decoder_AttConditionalLSTMCond[0]\n",
            "__________________________________________________________________________________________________\n",
            "logit_ctx (TimeDistributed)     (None, None, 32)     2080        decoder_AttConditionalLSTMCond[0]\n",
            "__________________________________________________________________________________________________\n",
            "logit_lstm (TimeDistributed)    (None, None, 32)     1056        proj_h0_batch_normalization[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "permute_general_1 (PermuteGener (None, None, 32)     0           logit_ctx[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "logit_emb (TimeDistributed)     (None, None, 32)     1056        state_below_batch_normalization[0\n",
            "__________________________________________________________________________________________________\n",
            "out_layer_mlp_batch_normalizati (None, None, 32)     128         logit_lstm[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "out_layer_ctx_batch_normalizati (None, None, 32)     128         permute_general_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "out_layer_emb_batch_normalizati (None, None, 32)     128         logit_emb[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "additional_input (Add)          (None, None, 32)     0           out_layer_mlp_batch_normalization\n",
            "                                                                 out_layer_ctx_batch_normalization\n",
            "                                                                 out_layer_emb_batch_normalization\n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, 32)     0           additional_input[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "linear_0 (TimeDistributed)      (None, None, 32)     1056        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "out_layer_linear_0_batch_normal (None, None, 32)     128         linear_0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "target_text (TimeDistributed)   (None, None, 15460)  510180      out_layer_linear_0_batch_normaliz\n",
            "==================================================================================================\n",
            "Total params: 1,853,349\n",
            "Trainable params: 1,852,645\n",
            "Non-trainable params: 704\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /content/nmt-keras/nmt-keras/nmt_keras/model_zoo.py:213: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:57] From /content/nmt-keras/nmt-keras/nmt_keras/model_zoo.py:213: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "[20/06/2020 13:40:57] Preparing optimizer and compiling. Optimizer configuration: \n",
            "\t LR: 0.001\n",
            "\t LOSS: categorical_crossentropy\n",
            "\t BETA_1: 0.9\n",
            "\t BETA_2: 0.999\n",
            "\t EPSILON: 1e-08\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1192: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:40:57] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1192: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKQqWOneGD_3",
        "colab_type": "text"
      },
      "source": [
        "Next, we must define the inputs and outputs mapping from our Dataset instance to our model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqEqWxYHGIM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "inputMapping = dict()\n",
        "for i, id_in in enumerate(params['INPUTS_IDS_DATASET']):\n",
        "    pos_source = dataset.ids_inputs.index(id_in)\n",
        "    id_dest = nmt_model.ids_inputs[i]\n",
        "    inputMapping[id_dest] = pos_source\n",
        "nmt_model.setInputsMapping(inputMapping)\n",
        "\n",
        "outputMapping = dict()\n",
        "for i, id_out in enumerate(params['OUTPUTS_IDS_DATASET']):\n",
        "    pos_target = dataset.ids_outputs.index(id_out)\n",
        "    id_dest = nmt_model.ids_outputs[i]\n",
        "    outputMapping[id_dest] = pos_target\n",
        "nmt_model.setOutputsMapping(outputMapping)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKPW2hv8GKMj",
        "colab_type": "text"
      },
      "source": [
        "We can add some callbacks for controlling the training (e.g. Sampling each N updates, early stop, learning rate annealing...). For instance, let's build a sampling callback. After each epoch, it will compute the BLEU scores on the development set using the sacreBLEU package. We need to pass some configuration variables to the callback (in the extra_vars dictionary):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MtMvSoAGNHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_transformer = params.get('ATTEND_ON_OUTPUT', 'transformer' in params['MODEL_TYPE'].lower())\n",
        "search_params = {\n",
        "    'language': 'en',\n",
        "    'tokenize_f': eval('dataset.' + 'tokenize_none'),\n",
        "    'beam_size': 12,\n",
        "    'optimized_search': True,\n",
        "    'model_inputs': params['INPUTS_IDS_MODEL'],\n",
        "    'model_outputs': params['OUTPUTS_IDS_MODEL'],\n",
        "    'dataset_inputs':  params['INPUTS_IDS_DATASET'],\n",
        "    'dataset_outputs':  params['OUTPUTS_IDS_DATASET'],\n",
        "    'n_parallel_loaders': 1,\n",
        "    'maxlen': 50,\n",
        "    'normalize_probs': True,\n",
        "    'pos_unk': True and not is_transformer,  # Pos_unk is unimplemented for transformer models\n",
        "    'heuristic': 0,\n",
        "    'state_below_maxlen': -1,\n",
        "    'attend_on_output': is_transformer,\n",
        "    'val': {'references': dataset.extra_variables['val']['target_text']}\n",
        "  }\n",
        "\n",
        "vocab = dataset.vocabulary['target_text']['idx2words']\n",
        "callbacks = []\n",
        "input_text_id = params['INPUTS_IDS_DATASET'][0]\n",
        "\n",
        "callbacks.append(PrintPerformanceMetricOnEpochEndOrEachNUpdates(nmt_model,\n",
        "                                                                dataset,\n",
        "                                                                gt_id='target_text',\n",
        "                                                                metric_name=['sacrebleu'],\n",
        "                                                                set_name=['val'],\n",
        "                                                                batch_size=50,\n",
        "                                                                each_n_epochs=1,\n",
        "                                                                extra_vars=search_params,\n",
        "                                                                reload_epoch=0,\n",
        "                                                                is_text=True,\n",
        "                                                                input_text_id=input_text_id,\n",
        "                                                                index2word_y=vocab,\n",
        "                                                                sampling_type='max_likelihood',\n",
        "                                                                beam_search=True,\n",
        "                                                                save_path=nmt_model.model_path,\n",
        "                                                                start_eval_on_epoch=0,\n",
        "                                                                write_samples=True,\n",
        "                                                                write_type='list',\n",
        "                                                                verbose=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo-kOSLlGQux",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to train. Let's set up some training parameters...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-oborMLGUMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_params = {'n_epochs': 8,\n",
        "                   'batch_size': 50,\n",
        "                   'maxlen': 30,\n",
        "                   'epochs_for_save': 1,\n",
        "                   'verbose': 1,\n",
        "                   'eval_on_sets': [], \n",
        "                   'n_parallel_loaders': 1,\n",
        "                   'extra_callbacks': callbacks,\n",
        "                   'reload_epoch': 0,\n",
        "                   'epoch_offset': 0}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7m3oR5RGVDp",
        "colab_type": "text"
      },
      "source": [
        "And train!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcyAKL4cGai4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "836e5628-acf2-4f57-cdb4-0272a55b471c"
      },
      "source": [
        "nmt_model.trainNet(dataset, training_params)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:41:35] <<< Training model >>>\n",
            "[20/06/2020 13:41:35] Training parameters: { \n",
            "\tbatch_size: 50\n",
            "\tclass_weights: None\n",
            "\tda_enhance_list: []\n",
            "\tda_patch_type: resize_and_rndcrop\n",
            "\tdata_augmentation: False\n",
            "\teach_n_epochs: 1\n",
            "\tepoch_offset: 0\n",
            "\tepochs_for_save: 1\n",
            "\teval_on_epochs: True\n",
            "\teval_on_sets: []\n",
            "\textra_callbacks: [<keras_wrapper.extra.callbacks.EvalPerformance object at 0x7f0109ae2b70>]\n",
            "\thomogeneous_batches: False\n",
            "\tinitial_lr: 1.0\n",
            "\tjoint_batches: 4\n",
            "\tlr_decay: None\n",
            "\tlr_gamma: 0.1\n",
            "\tlr_half_life: 50000\n",
            "\tlr_reducer_exp_base: 0.5\n",
            "\tlr_reducer_type: linear\n",
            "\tlr_warmup_exp: -1.5\n",
            "\tmaxlen: 30\n",
            "\tmean_substraction: False\n",
            "\tmetric_check: None\n",
            "\tmin_delta: 0.0\n",
            "\tmin_lr: 1e-09\n",
            "\tn_epochs: 8\n",
            "\tn_gpus: 1\n",
            "\tn_parallel_loaders: 1\n",
            "\tnormalization_type: None\n",
            "\tnormalize: False\n",
            "\tnum_iterations_val: None\n",
            "\tpatience: 0\n",
            "\tpatience_check_split: val\n",
            "\treduce_each_epochs: True\n",
            "\treload_epoch: 0\n",
            "\tshuffle: True\n",
            "\tstart_eval_on_epoch: 0\n",
            "\tstart_reduction_on_epoch: 0\n",
            "\ttensorboard: False\n",
            "\ttensorboard_params: {'log_dir': 'tensorboard_logs', 'histogram_freq': 0, 'batch_size': 50, 'write_graph': True, 'write_grads': False, 'write_images': False, 'embeddings_freq': 0, 'embeddings_layer_names': None, 'embeddings_metadata': None, 'update_freq': 'epoch'}\n",
            "\tverbose: 1\n",
            "\two_da_patch_type: whole\n",
            "}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3315: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:41:40] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3315: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:292: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:41:40] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:292: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:299: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:41:40] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:299: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:312: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:41:40] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:312: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:321: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:41:40] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:321: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:328: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:41:40] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:328: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "580/580 [==============================] - 504s 869ms/step - loss: 3.1350 - perplexity: 6324.2339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:50:05] <<< Saving model to trained_models/tutorial_model/epoch_1 ... >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "  'TensorFlow optimizers do not '\n",
            "[20/06/2020 13:50:08] <<< Model saved >>>\n",
            "\n",
            "[20/06/2020 13:50:08] <<< Predicting outputs of val set >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Total cost: 11301.798203 \t Average cost: 11.145758\n",
            "The sampling took: 208.869295 secs (Speed: 0.205985 sec/sample)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:53:37] Prediction output 0: target_text (text)\n",
            "[20/06/2020 13:53:37] Decoding beam search prediction ...\n",
            "[20/06/2020 13:53:37] Using heuristic 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 13:53:37] Evaluating on metric sacrebleu\n",
            "[20/06/2020 13:53:37] Computing SacreBleu scores on the val split...\n",
            "[20/06/2020 13:53:37] Bleu_4: 0.34382451706233275\n",
            "[20/06/2020 13:53:37] Done evaluating on metric sacrebleu\n",
            "[20/06/2020 13:53:37] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_1.jpg >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/8\n",
            "580/580 [==============================] - 505s 871ms/step - loss: 2.1734 - perplexity: 6868.8950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:02:02] <<< Saving model to trained_models/tutorial_model/epoch_2 ... >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:02:03] <<< Model saved >>>\n",
            "\n",
            "[20/06/2020 14:02:03] <<< Predicting outputs of val set >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Total cost: 18548.678641 \t Average cost: 18.292582\n",
            "The sampling took: 169.148276 secs (Speed: 0.166813 sec/sample)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:04:52] Prediction output 0: target_text (text)\n",
            "[20/06/2020 14:04:52] Decoding beam search prediction ...\n",
            "[20/06/2020 14:04:52] Using heuristic 0\n",
            "[20/06/2020 14:04:52] Evaluating on metric sacrebleu\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:04:52] Computing SacreBleu scores on the val split...\n",
            "[20/06/2020 14:04:52] Bleu_4: 13.996877467559987\n",
            "[20/06/2020 14:04:52] Done evaluating on metric sacrebleu\n",
            "[20/06/2020 14:04:53] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_2.jpg >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/8\n",
            "580/580 [==============================] - 503s 867ms/step - loss: 1.8448 - perplexity: 7184.7231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:13:15] <<< Saving model to trained_models/tutorial_model/epoch_3 ... >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:13:16] <<< Model saved >>>\n",
            "\n",
            "[20/06/2020 14:13:16] <<< Predicting outputs of val set >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Total cost: 14501.026911 \t Average cost: 14.300815\n",
            "The sampling took: 157.329790 secs (Speed: 0.155158 sec/sample)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:15:53] Prediction output 0: target_text (text)\n",
            "[20/06/2020 14:15:53] Decoding beam search prediction ...\n",
            "[20/06/2020 14:15:53] Using heuristic 0\n",
            "[20/06/2020 14:15:53] Evaluating on metric sacrebleu\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:15:54] Computing SacreBleu scores on the val split...\n",
            "[20/06/2020 14:15:54] Bleu_4: 16.729644635441666\n",
            "[20/06/2020 14:15:54] Done evaluating on metric sacrebleu\n",
            "[20/06/2020 14:15:54] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_3.jpg >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/8\n",
            "580/580 [==============================] - 510s 879ms/step - loss: 1.6330 - perplexity: 7159.2695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:24:23] <<< Saving model to trained_models/tutorial_model/epoch_4 ... >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:24:24] <<< Model saved >>>\n",
            "\n",
            "[20/06/2020 14:24:24] <<< Predicting outputs of val set >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Total cost: 12796.827810 \t Average cost: 12.620146\n",
            "The sampling took: 140.501818 secs (Speed: 0.138562 sec/sample)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:26:44] Prediction output 0: target_text (text)\n",
            "[20/06/2020 14:26:44] Decoding beam search prediction ...\n",
            "[20/06/2020 14:26:44] Using heuristic 0\n",
            "[20/06/2020 14:26:45] Evaluating on metric sacrebleu\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:26:45] Computing SacreBleu scores on the val split...\n",
            "[20/06/2020 14:26:45] Bleu_4: 22.648543921228203\n",
            "[20/06/2020 14:26:45] Done evaluating on metric sacrebleu\n",
            "[20/06/2020 14:26:45] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_4.jpg >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/8\n",
            "580/580 [==============================] - 501s 865ms/step - loss: 1.4990 - perplexity: 7319.3677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:35:06] <<< Saving model to trained_models/tutorial_model/epoch_5 ... >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:35:07] <<< Model saved >>>\n",
            "\n",
            "[20/06/2020 14:35:07] <<< Predicting outputs of val set >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Total cost: 12474.627965 \t Average cost: 12.302394\n",
            "The sampling took: 137.670628 secs (Speed: 0.135770 sec/sample)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:37:25] Prediction output 0: target_text (text)\n",
            "[20/06/2020 14:37:25] Decoding beam search prediction ...\n",
            "[20/06/2020 14:37:25] Using heuristic 0\n",
            "[20/06/2020 14:37:25] Evaluating on metric sacrebleu\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:37:25] Computing SacreBleu scores on the val split...\n",
            "[20/06/2020 14:37:25] Bleu_4: 25.985609476394615\n",
            "[20/06/2020 14:37:25] Done evaluating on metric sacrebleu\n",
            "[20/06/2020 14:37:25] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_5.jpg >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6/8\n",
            "580/580 [==============================] - 500s 862ms/step - loss: 1.4051 - perplexity: 7149.3105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:45:45] <<< Saving model to trained_models/tutorial_model/epoch_6 ... >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:45:46] <<< Model saved >>>\n",
            "\n",
            "[20/06/2020 14:45:46] <<< Predicting outputs of val set >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Total cost: 10634.221109 \t Average cost: 10.487398\n",
            "The sampling took: 132.722480 secs (Speed: 0.130890 sec/sample)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:47:59] Prediction output 0: target_text (text)\n",
            "[20/06/2020 14:47:59] Decoding beam search prediction ...\n",
            "[20/06/2020 14:47:59] Using heuristic 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:47:59] Evaluating on metric sacrebleu\n",
            "[20/06/2020 14:47:59] Computing SacreBleu scores on the val split...\n",
            "[20/06/2020 14:47:59] Bleu_4: 26.377897676934694\n",
            "[20/06/2020 14:47:59] Done evaluating on metric sacrebleu\n",
            "[20/06/2020 14:47:59] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_6.jpg >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/8\n",
            "580/580 [==============================] - 496s 855ms/step - loss: 1.3420 - perplexity: 7007.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:56:15] <<< Saving model to trained_models/tutorial_model/epoch_7 ... >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:56:15] <<< Model saved >>>\n",
            "\n",
            "[20/06/2020 14:56:15] <<< Predicting outputs of val set >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Total cost: 10016.882394 \t Average cost: 9.878582\n",
            "The sampling took: 128.586598 secs (Speed: 0.126811 sec/sample)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:58:24] Prediction output 0: target_text (text)\n",
            "[20/06/2020 14:58:24] Decoding beam search prediction ...\n",
            "[20/06/2020 14:58:24] Using heuristic 0\n",
            "[20/06/2020 14:58:24] Evaluating on metric sacrebleu\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 14:58:24] Computing SacreBleu scores on the val split...\n",
            "[20/06/2020 14:58:24] Bleu_4: 29.452345864699875\n",
            "[20/06/2020 14:58:24] Done evaluating on metric sacrebleu\n",
            "[20/06/2020 14:58:24] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_7.jpg >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/8\n",
            "580/580 [==============================] - 505s 871ms/step - loss: 1.2804 - perplexity: 6349.5278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 15:06:50] <<< Saving model to trained_models/tutorial_model/epoch_8 ... >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 15:06:51] <<< Model saved >>>\n",
            "\n",
            "[20/06/2020 15:06:51] <<< Predicting outputs of val set >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Total cost: 9412.687276 \t Average cost: 9.282729\n",
            "The sampling took: 125.681089 secs (Speed: 0.123946 sec/sample)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 15:08:56] Prediction output 0: target_text (text)\n",
            "[20/06/2020 15:08:56] Decoding beam search prediction ...\n",
            "[20/06/2020 15:08:56] Using heuristic 0\n",
            "[20/06/2020 15:08:56] Evaluating on metric sacrebleu\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 15:08:57] Computing SacreBleu scores on the val split...\n",
            "[20/06/2020 15:08:57] Bleu_4: 29.745362978969368\n",
            "[20/06/2020 15:08:57] Done evaluating on metric sacrebleu\n",
            "[20/06/2020 15:08:57] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_8.jpg >>>\n",
            "[20/06/2020 15:08:57] <<< Finished training model >>>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq3_MyX3A4XV",
        "colab_type": "text"
      },
      "source": [
        "## 3. Decoding with a trained Neural Machine Translation Model\n",
        "\n",
        "Now, we'll load from disk the model we just trained and we'll apply it for translating new text. In this case, we want to translate the 'test' split from our dataset.\n",
        "\n",
        "Since we want to translate a new data split ('test') we must add it to the dataset instance, just as we did before (at the first tutorial). In case we also had the refences of the test split and we wanted to evaluate it, we can add it to the dataset. Note that this is not mandatory and we could just predict without evaluating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H-jXRq4BGm_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "565735ce-345d-42a1-9293-b7525af7c4fe"
      },
      "source": [
        "dataset.setInput('/content/drive/My Drive/parallel-dataset/test_2017_flickr.de',\n",
        "            'test',\n",
        "            type='text',\n",
        "            id='source_text',\n",
        "            pad_on_batch=True,\n",
        "            tokenization='tokenize_none',\n",
        "            fill='end',\n",
        "            max_text_len=50,\n",
        "            min_occ=0)\n",
        "\n",
        "dataset.setInput(None,\n",
        "            'test',\n",
        "            type='ghost',\n",
        "            id='state_below',\n",
        "            required=False)\n",
        "\n",
        "dataset.setRawInput('/content/drive/My Drive/parallel-dataset/test_2017_flickr.de',\n",
        "              'test',\n",
        "              type='file-name',\n",
        "              id='raw_source_text',\n",
        "              overwrite_split=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 15:11:16] \tApplying tokenization function: \"tokenize_none\".\n",
            "[20/06/2020 15:11:16] Loaded \"test\" set inputs of data_type \"text\" with data_id \"source_text\" and length 1000.\n",
            "[20/06/2020 15:11:16] Loaded \"test\" set inputs of data_type \"ghost\" with data_id \"state_below\" and length 1000.\n",
            "[20/06/2020 15:11:16] Loaded \"test\" set inputs of type \"file-name\" with id \"raw_source_text\".\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUZveIgLCzlq",
        "colab_type": "text"
      },
      "source": [
        "Now, let's load the translation model. Suppose we want to load the model saved at the end of the epoch 4:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hgOSknZC2lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "119021e8-2825-41fc-dddd-46f83d21039d"
      },
      "source": [
        "params['INPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len[params['INPUTS_IDS_DATASET'][0]]\n",
        "params['OUTPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len[params['OUTPUTS_IDS_DATASET'][0]]\n",
        "\n",
        "# Load model\n",
        "nmt_model = loadModel('trained_models/tutorial_model', 8)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 15:11:24] <<< Loading model from trained_models/tutorial_model/epoch_8_Model_Wrapper.pkl ... >>>\n",
            "[20/06/2020 15:11:24] <<< Loading model from trained_models/tutorial_model/epoch_8.h5 ... >>>\n",
            "[20/06/2020 15:11:27] <<< Loading optimized model... >>>\n",
            "[20/06/2020 15:11:32] <<< Optimized model loaded. >>>\n",
            "[20/06/2020 15:11:32] <<< Model loaded in 7.7300 seconds. >>>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piDc_y0pC5la",
        "colab_type": "text"
      },
      "source": [
        "Once we loaded the model, we just have to invoke the sampling method (in this case, the Beam Search algorithm) for the 'test' split:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FBT1HWYC9ip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "2d2b949c-4331-4d00-eeac-aabd114c7a21"
      },
      "source": [
        "is_transformer = params.get('ATTEND_ON_OUTPUT', 'transformer' in params['MODEL_TYPE'].lower())\n",
        "\n",
        "params_prediction = {\n",
        "    'language': 'en',\n",
        "    'tokenize_f': eval('dataset.' + 'tokenize_none'),\n",
        "    'beam_size': 12,\n",
        "    'optimized_search': True,\n",
        "    'model_inputs': params['INPUTS_IDS_MODEL'],\n",
        "    'model_outputs': params['OUTPUTS_IDS_MODEL'],\n",
        "    'dataset_inputs':  params['INPUTS_IDS_DATASET'],\n",
        "    'dataset_outputs':  params['OUTPUTS_IDS_DATASET'],\n",
        "    'n_parallel_loaders': 1,\n",
        "    'maxlen': 50,\n",
        "    'normalize_probs': True,\n",
        "    'pos_unk': True and not is_transformer,\n",
        "    'heuristic': 0,\n",
        "    'state_below_maxlen': -1,\n",
        "    'predict_on_sets': ['test'],\n",
        "    'verbose': 0,\n",
        "    'attend_on_output': is_transformer\n",
        "  }\n",
        "predictions = nmt_model.predictBeamSearchNet(dataset, params_prediction)['test']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[20/06/2020 15:11:39] <<< Predicting outputs of test set >>>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Total cost: 8872.210595 \t Average cost: 8.872211\n",
            "The sampling took: 117.405943 secs (Speed: 0.117406 sec/sample)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2JcEpFJDTDs",
        "colab_type": "text"
      },
      "source": [
        "Up to now, in the variable 'predictions', we have the indices of the words of the hypotheses. We must decode them into words. For doing this, we'll use the dictionary stored in the dataset object:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EGTAOFXDYLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53cc8e38-d38a-4520-b0ff-87351b8cdef1"
      },
      "source": [
        "from keras_wrapper.utils import decode_predictions_beam_search\n",
        "vocab = dataset.vocabulary['target_text']['idx2words']\n",
        "samples = predictions['samples'] # Get word indices from the samples.\n",
        "\n",
        "predictions = decode_predictions_beam_search(samples,  \n",
        "                                             vocab,\n",
        "                                             verbose=params['VERBOSE'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 15:14:17] Decoding beam search prediction ...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MZVhj0IDd93",
        "colab_type": "text"
      },
      "source": [
        "Finally, we store the hypotheses:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kznqPYZMDg8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1a9999ed-d2c6-4b19-acc8-9b5632746e2b"
      },
      "source": [
        "filepath = 'test.pred'\n",
        "from keras_wrapper.extra.read_write import list2file\n",
        "list2file(filepath, predictions)\n",
        "!head -n 4 test.pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A sidewalk.\n",
            "A guard dog and one is getting out of an ice cream truck in a Asian car.\n",
            "A athlete with brown gear in front of a graffiti covered in front of a tree that is holding a mountain parked by the sun.\n",
            "A man with a naked pants swings on a music match on a stage.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUntF5T6Dx2w",
        "colab_type": "text"
      },
      "source": [
        "If we have the references of this split, we can also evaluate the performance of our system on it. First, we must add them to the dataset object:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pccriZWDyqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "a14bc171-2d04-48ef-b1a0-8eee666837ec"
      },
      "source": [
        "dataset.setOutput('/content/drive/My Drive/parallel-dataset/test_2017_flickr.en',\n",
        "             'test',\n",
        "             type='text',\n",
        "             id='target_text',\n",
        "             pad_on_batch=True,\n",
        "             tokenization='tokenize_none',\n",
        "             sample_weights=True,\n",
        "             max_text_len=30,\n",
        "             max_words=0)\n",
        "keep_n_captions(dataset, repeat=1, n=1, set_names=['test'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 15:14:29] \tApplying tokenization function: \"tokenize_none\".\n",
            "[20/06/2020 15:14:29] Loaded \"test\" set outputs of data_type \"text\" with data_id \"target_text\" and length 1000.\n",
            "[20/06/2020 15:14:29] Keeping 1 captions per input on the test set.\n",
            "[20/06/2020 15:14:29] Samples reduced to 1000 in test set.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riPUDl-xD1WM",
        "colab_type": "text"
      },
      "source": [
        "Next, we call the evaluation system: the sacreBLEU package:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfLzm4QBD2oj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e31e94cd-d4e9-4428-db91-802978663b2e"
      },
      "source": [
        "\n",
        "from keras_wrapper.extra.evaluation import select\n",
        "metric = 'sacrebleu'\n",
        "# Apply sampling\n",
        "extra_vars = dict()\n",
        "extra_vars['tokenize_f'] = eval('dataset.' + 'tokenize_none')\n",
        "extra_vars['language'] = params['TRG_LAN']\n",
        "extra_vars['test'] = dict()\n",
        "extra_vars['test']['references'] = dataset.extra_variables['test']['target_text']\n",
        "metrics = select[metric](pred_list=predictions,\n",
        "                                          verbose=1,\n",
        "                                          extra_vars=extra_vars,\n",
        "                                          split='test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20/06/2020 15:14:37] Computing SacreBleu scores on the test split...\n",
            "[20/06/2020 15:14:37] Bleu_4: 23.304958245214795\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtfESZgUH6g_",
        "colab_type": "text"
      },
      "source": [
        "And that's all!"
      ]
    }
  ]
}